{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import sys, os\n",
    "import time\n",
    "import dask\n",
    "import multiprocessing\n",
    "ncpus = multiprocessing.cpu_count()\n",
    "#from BurnCube import BurnCube #including burn mapping main functions\n",
    "#bc = BurnCube()\n",
    "\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "\n",
    "import datacube\n",
    "from datacube.storage import masking\n",
    "#from datacube.storage.masking import mask_to_dict\n",
    "from datacube.storage.storage import write_dataset_to_netcdf\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "dc = datacube.Datacube(app='dc_burnmap_comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/g/data/r78/DPIPWE_lm/test_burn_mapping/output_data'\n",
    "if not os.path.exists(outputdir):\n",
    "    print(\"output directory doesn't exist\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = True\n",
    "label = '12,-47'\n",
    "\n",
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "albers = gpd.read_file('/g/data/r78/DPIPWE_lm/test_burn_mapping/reference_data/Albers_Australia_Coast_Islands_Reefs.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on tile 12,-47...\n"
     ]
    }
   ],
   "source": [
    "if label:\n",
    "    index = albers[albers['label']==label].index[0]\n",
    "    x = (albers.loc[index]['X_MIN'], albers.loc[index]['X_MAX'])\n",
    "    y = (albers.loc[index]['Y_MIN'], albers.loc[index]['Y_MAX'])\n",
    "    output_filename = outputdir + '/composite_2016-2017_'+'_'.join(label.split(','))+'.nc'\n",
    "    print(\"Working on tile {}...\".format(label))\n",
    "else:\n",
    "    x, y = (1385000.0, 1375000.0), (-4570000.0, -4580000.0)\n",
    "    if subset:\n",
    "        output_filename = 'composite_2016-2017_test_subset.nc'\n",
    "    else:\n",
    "        output_filename = 'composite_2016-2017_test_one.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = 'ls8'#_nbart_albers'\n",
    "#datatime = ('2017-01-01', '2017-01-30') # period to retrieve data\n",
    "#referenceperiod = ('2013-01-01', '2016-06-30') # period used for the calculation of geometric median\n",
    "#mappingperiod = ('2016-07-01', '2017-06-30') # period of interest for change/severity mapping\n",
    "#res = (25, 25)\n",
    "\n",
    "query = {'x': x,\n",
    "         'y': y,\n",
    "         'time': ('2017-01-01', '2017-01-30'),\n",
    "         'resolution': (250,250),\n",
    "         'crs': 'EPSG:3577'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.monotonic()\n",
    "#try:\n",
    "#    data = bc.load_cube(x, y, res, datatime, [sensor])\n",
    "#except:\n",
    "#    print(\"Problem loading data\")\n",
    "#    exit()\n",
    "#print(\"---{} minutes for loading data.---\".format((time.monotonic()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PQ data for same query used to load Landsat data\n",
    "#pq_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def burncomp(x, y):\n",
    "    ds = dc.load(product=sensor+'_nbart_albers', group_by = 'solar_day', dask_chunks={'time': 1}, **query)\n",
    "    \n",
    "    # Load PQ data for same query used to load Landsat data\n",
    "    pq_ds = dc.load(product = sensor+'_pq_albers',\n",
    "                    group_by = 'solar_day',\n",
    "                    fuse_func=ga_pq_fuser,\n",
    "                    dask_chunks={'time': 1},\n",
    "                    **query)\n",
    "\n",
    "    # Use PQ to create mask that is True for pixels that are not affected by clouds, cloud shadow or saturation\n",
    "    good_quality_ds = masking.make_mask(pq_ds.pixelquality,\n",
    "                                    cloud_acca='no_cloud',\n",
    "                                    cloud_fmask='no_cloud',\n",
    "                                    cloud_shadow_acca='no_cloud_shadow',\n",
    "                                    cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                    blue_saturated=False,\n",
    "                                    green_saturated=False,\n",
    "                                    red_saturated=False,\n",
    "                                    nir_saturated=False,\n",
    "                                    swir1_saturated=False,\n",
    "                                    swir2_saturated=False,\n",
    "                                    contiguous=True)\n",
    "\n",
    "    # Remove -999 nodata values prior to analysing or plotting Landsat imagery by setting all nodata values to `NaN`\n",
    "    ds = masking.mask_invalid_data(ds)\n",
    "\n",
    "    # Apply the mask to preserve only the good data\n",
    "    ds = ds.where(good_quality_ds) \n",
    "    \n",
    "    # compute geomedian\n",
    "    out = GeoMedian().compute(ds)\n",
    "    return out.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geomedian_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm, ym = (x[0]+x[1])/2, (y[0]+y[1])/2\n",
    "x1, x2 = (x[0], xm), (xm, x[1])\n",
    "y1, y2 = (y[0], ym), (ym, y[1])\n",
    "if subset:\n",
    "    out1 = burncomp(x1, y)\n",
    "    #out2 = burncomp(x2, y)\n",
    "    #out = xr.concat([out1, out2], dim='x')\n",
    "else:\n",
    "    out1 = burnmap(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:          (x: 400, y: 400)\n",
       "Coordinates:\n",
       "  * y                (y) float64 -4.7e+06 -4.7e+06 ... -4.6e+06 -4.6e+06\n",
       "  * x                (x) float64 1.2e+06 1.2e+06 1.201e+06 ... 1.3e+06 1.3e+06\n",
       "Data variables:\n",
       "    coastal_aerosol  (y, x) float64 nan nan nan nan ... 181.8 269.0 191.0 210.0\n",
       "    blue             (y, x) float64 nan nan nan nan ... 142.8 226.0 150.0 172.2\n",
       "    green            (y, x) float64 nan nan nan nan ... 239.4 357.0 252.5 307.1\n",
       "    red              (y, x) float64 nan nan nan nan ... 265.4 342.0 230.0 272.3\n",
       "    nir              (y, x) float64 nan nan nan ... 1.864e+03 2.234e+03\n",
       "    swir1            (y, x) float64 nan nan nan ... 1.306e+03 904.0 1.048e+03\n",
       "    swir2            (y, x) float64 nan nan nan nan ... 441.7 626.0 425.5 515.3\n",
       "Attributes:\n",
       "    crs:      EPSG:3577"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Datacube' object has no attribute 'storage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c9963c0c9bce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Output to netcdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_dataset_to_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/g/data/r78/DPIPWE_lm/test_burn_mapping/output_data/out_test_oct.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#out1.geomed.to_netcdf('out_test_oct.nc')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Datacube' object has no attribute 'storage'"
     ]
    }
   ],
   "source": [
    "# geomedian_ds = GeoMedian().compute(out)\n",
    "\n",
    "# Output to netcdf\n",
    "datacube.storage.storage.write_dataset_to_netcdf(out1, '/g/data/r78/DPIPWE_lm/test_burn_mapping/output_data/out_test_oct.nc')\n",
    "#out1.geomed.to_netcdf('out_test_oct.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
